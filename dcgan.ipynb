{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheodorSergeev/optml_gan/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fe1zK4TEn2P"
      },
      "source": [
        "Adapted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElE3epO2FCLq"
      },
      "source": [
        "# Initialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1MBmvsdWEd",
        "outputId": "2c66a80f-d454-420c-e9ab-24b361acc456"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # packages to generate requirement.txt\n",
        "    %pip install nbconvert\n",
        "    %pip install pipreqs\n",
        "    # for Frechet inception distance\n",
        "    %pip install pytorch-fid\n",
        "\n",
        "    %cd drive/My Drive/optml_gan2\n",
        "    PATH = './'\n",
        "else:\n",
        "    PATH = './'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M50Mkga4EhNy"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQY6pqVIgdUH"
      },
      "source": [
        "# Generate Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YaRkKzQNgdUH"
      },
      "outputs": [],
      "source": [
        "# # converts notebook to .py file for pipreqs\n",
        "# !jupyter nbconvert --output-dir=\"./\" --to script dcgan.ipynb\n",
        "\n",
        "# # creates the requirement.txt file\n",
        "# !pipreqs --force\n",
        "# os.remove('./dcgan.py')  # deletes the .py file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqmZqleaEhN0"
      },
      "source": [
        "# Source code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.data_handling import *\n",
        "from src.utils import *\n",
        "from src.model import *\n",
        "from src.losses import *\n",
        "\n",
        "\n",
        "loss_dict = {\n",
        "    \"kl\": (loss_dis_kl, loss_gen_kl),\n",
        "    \"wass\": (loss_dis_wasser, loss_gen_wasser),\n",
        "    \"hinge\": (loss_dis_hinge, loss_gen_hinge)\n",
        "}\n",
        "\n",
        "# FID\n",
        "\n",
        "from src.training import *\n",
        "from src.visualisation import *\n",
        "from src.serialisation import *\n",
        "\n",
        "# https://keras.io/examples/generative/conditional_gan/\n",
        "from src.architectures import *\n",
        "\n",
        "from src.gridsearch import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwd6NVO7zoze"
      },
      "source": [
        "## FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJpwdqHN1lN-"
      },
      "source": [
        "https://www.kaggle.com/code/ibtesama/gan-in-pytorch-with-fid/notebook  \n",
        "https://github.com/mseitzer/pytorch-fid   \n",
        "Currently uses the Kaggle stuff since calculating FID on the repo is run from the command line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "YxtQ5pk4z_0M"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "fIkLqEY-zpQm"
      },
      "outputs": [],
      "source": [
        "def calculate_activation_statistics(images, inception_model, batch_size=128, dims=2048, cuda=False):\n",
        "    inception_model.eval()\n",
        "    act = np.empty((len(images), dims))\n",
        "\n",
        "    if cuda:\n",
        "        batch = images.cuda()\n",
        "    else:\n",
        "        batch = images\n",
        "\n",
        "    pred = inception_model(batch)[0]\n",
        "\n",
        "    # If model output is not scalar, apply global spatial average pooling.\n",
        "    # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act = pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "heCyV4qHzu3D"
      },
      "outputs": [],
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "uBzRjX6Bzu06"
      },
      "outputs": [],
      "source": [
        "def calculate_frechet(images_real, images_fake, model):\n",
        "    mu_1, std_1 = calculate_activation_statistics(images_real, model, cuda=True)\n",
        "    mu_2, std_2 = calculate_activation_statistics(images_fake, model, cuda=True)\n",
        "\n",
        "    # Get Frechet distance\n",
        "    fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "    return fid_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIukUkR5ny_"
      },
      "source": [
        "# Hyperparameter optimisation (gridsearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NTdcMZ4AtiVw"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = PATH + \"data/\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = 'mnist'  # 'cifar10' or 'mnist'\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this size using a transformer\n",
        "image_size = 28  # 28 for mnist, 64 for others\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 128\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_repo_paths(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DRqoAVcKtC05"
      },
      "outputs": [],
      "source": [
        "dataset, nc = get_dataset(dataset_name, image_size, dataroot)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEmdQBI_LFcW",
        "outputId": "e780fcb6-e77b-44bf-b914-5f02fba4471e"
      },
      "outputs": [],
      "source": [
        "grid_search(ngpu, device, dataset, workers,\n",
        "            experiment_prefix='',           # add an extra word at the begining to the save path of the models and stats\n",
        "            batch_size_list=[128],\n",
        "            shuffle_list=[True],\n",
        "            num_epochs_list=[300],\n",
        "            loss_name_list=['wass'],        # wass, hinge\n",
        "            optimizer_name_list=['adam'],   # 'adam' 'sgd' 'rmsprop'\n",
        "            beta1_list=[0.9],               # 0.9 == default # Beta1 hyperparam for Adam optimizers\n",
        "            lr_list=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
        "            momentums_list=[(0, 0)],        # [(momentumD, momentumG)]\n",
        "            plot=False,\n",
        "            save_stats=True,                # save the stats to disk\n",
        "            create_dir=True,                # create the directories to save files\n",
        "            save_epochs=10,                 # save the model every save_epochs epochs\n",
        "            save_models=True,               # save the models to disk\n",
        "            manualSeed=123,                 # keep at 123\n",
        "            nc=nc, nz=nz\n",
        "            )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search(ngpu, device, dataset, workers,\n",
        "            experiment_prefix='',           # add an extra word at the begining to the save path of the models and stats\n",
        "            batch_size_list=[128],\n",
        "            shuffle_list=[True],\n",
        "            num_epochs_list=[300],\n",
        "            loss_name_list=['wass'],        # wass, hinge\n",
        "            optimizer_name_list=['sgd'],    # 'adam' 'sgd' 'rmsprop'\n",
        "            beta1_list=[0.9],               # 0.9 == default # Beta1 hyperparam for Adam optimizers\n",
        "            lr_list=[1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7],\n",
        "            momentums_list=[(0, 0)],        # [(momentumD, momentumG)]\n",
        "            plot=False,\n",
        "            save_stats=True,                # save the stats to disk\n",
        "            create_dir=True,                # create the directories to save files\n",
        "            save_epochs=10,                 # save the model every save_epochs epochs\n",
        "            save_models=True,               # save the models to disk\n",
        "            manualSeed=123,                 # keep at 123\n",
        "            nc=nc, nz=nz\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beMBvkIjFl7z"
      },
      "source": [
        "# Training example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSQF8-Mitrs2"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeYrHwkRx-59"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = PATH + \"data/\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = 'mnist'  # 'cifar10' or 'mnist'\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
        "image_size = 28  # 28 for mnist, 64 for others\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 128\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROhmdtOztq8W"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs\n",
        "num_epochs = 3\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lrD = 2e-4\n",
        "lrG = 2e-4\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.9  # 0.9 == default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvDLoiTmyC8b"
      },
      "outputs": [],
      "source": [
        "dataset, nc = get_dataset(dataset_name, image_size, dataroot)\n",
        "\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rZFBRsvj-e"
      },
      "outputs": [],
      "source": [
        "loss_name = \"wass\"  # wass, hinge\n",
        "iter_dis, iter_gen, grad_penalty_coef = 1, 1, 0.0\n",
        "\n",
        "if loss_name == \"wass\":\n",
        "    iter_dis, grad_penalty_coef = 5, 10.0\n",
        "\n",
        "netG = init_net(Generator(ngpu, nc, nz), device, ngpu)\n",
        "print('Generator parameters', count_parameters(netG))\n",
        "\n",
        "netD = init_net(Discriminator(ngpu, nc, loss_name), device, ngpu)\n",
        "print('Discriminator parameters', count_parameters(netD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0wNXW77zD4b"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ-zyO5918Df"
      },
      "outputs": [],
      "source": [
        "fixed_noise, real_label, fake_label, optimizerD, optimizerG = init_optimizers(netD, netG, lrD, lrG, beta1, nz, device)\n",
        "experiment_prefix = ''  # add extra word to add the automatically generate one if you really need it, ideally keep empty\n",
        "gan_training = Training(loss_name, netD, netG, device, real_label, fake_label,\n",
        "                        dataloader, num_epochs, fixed_noise,\n",
        "                        grad_penalty_coef, lrD, lrG, beta1, experiment_prefix, save_models, PATH, save_stats=True, create_dir=True,\n",
        "                        iter_per_epoch_dis=1, iter_per_epoch_gen=1, grad_penalty_coef=0.0)\n",
        "\n",
        "stats = gan_training.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhfcOvw-GTCX"
      },
      "outputs": [],
      "source": [
        "img_list = stats['img_list']\n",
        "G_losses = stats['G_losses']\n",
        "D_losses = stats['D_losses']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-URPzv_TEhN7"
      },
      "source": [
        "# Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ9voI50JsfC"
      },
      "outputs": [],
      "source": [
        "plot_loss(G_losses, D_losses, PATH, save=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrhkQ1fwJrJa"
      },
      "outputs": [],
      "source": [
        "plot_realvsfake(dataloader, device, img_list, PATH, save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA4p6YNOEhN7"
      },
      "source": [
        "## Gâ€™s progression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI3SO9R1EhN7"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i, (1, 2, 0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaTJWVJPF1CQ"
      },
      "source": [
        "# Serialisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSHhTbJGBDMS"
      },
      "outputs": [],
      "source": [
        "epoch = 999999999\n",
        "experiment_prefix = ''\n",
        "experiment_path, stats_path, models_path = generate_paths(PATH, experiment_prefix, loss_name, lrD, lrG, beta1, iter_dis, iter_gen, grad_penalty_coef, create_dir=True)\n",
        "save_path_G, save_path_D = model_paths(experiment_path, epoch, models_path)\n",
        "\n",
        "print(experiment_path)\n",
        "print(stats_path)\n",
        "print(save_path_G)\n",
        "print(save_path_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OObYCZyxmO2a"
      },
      "outputs": [],
      "source": [
        "save_models(netG, netD, save_path_G, save_path_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nmoTYEInkMF"
      },
      "outputs": [],
      "source": [
        "pickle_save(stats, stats_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_MWpaCaKvxx"
      },
      "outputs": [],
      "source": [
        "# Load model\n",
        "netD, netG = load_models(ngpu, Discriminator, Generator, save_path_G, save_path_D, nc, nz, loss_name, device)\n",
        "\n",
        "stats = pickle_load(stats_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M21gaXFb05x8"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN_wInEf1trd"
      },
      "outputs": [],
      "source": [
        "def sample_gen(netG, nz, data):\n",
        "    with torch.no_grad():\n",
        "        real_cpu = data[0].to(device)\n",
        "        b_size = real_cpu.size(0)\n",
        "\n",
        "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "        fake = netG(noise)\n",
        "    return real_cpu, fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtDAYmuw04wd"
      },
      "outputs": [],
      "source": [
        "# Create the dataloader\n",
        "batch_size_eval = 100\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_eval,\n",
        "                                         shuffle=True, num_workers=workers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ndW2gBr6dAm"
      },
      "outputs": [],
      "source": [
        "# Load inception model\n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "inception_model = InceptionV3([block_idx])\n",
        "inception_model = inception_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG0cEgL71_6O"
      },
      "outputs": [],
      "source": [
        "# Take first batch from the dataloader to get 500 samples :\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(dataloader))\n",
        "\n",
        "    real_cpu, fake = sample_gen(netG, nz, sample_batch)\n",
        "\n",
        "    t_frechet = time.time()\n",
        "    frechet_dist = calculate_frechet(real_cpu, fake, inception_model)\n",
        "    print('frechet dist:', frechet_dist, '| time to calculate :', time.time()-t_frechet, 's')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-URPzv_TEhN7"
      ],
      "include_colab_link": true,
      "name": "dcgan.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('optml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "53461a9fb59a29bcaa7e1cfe6b9fe9c30f7f07aae145e77433631c890399cf7b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
