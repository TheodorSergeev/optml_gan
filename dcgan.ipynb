{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TheodorSergeev/optml_gan/blob/main/dcgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Fe1zK4TEn2P"
      },
      "source": [
        "Adapted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElE3epO2FCLq"
      },
      "source": [
        "# Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vg1MBmvsdWEd",
        "outputId": "2c66a80f-d454-420c-e9ab-24b361acc456"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:  \n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # packages to generate requirement.txt\n",
        "    %pip install nbconvert\n",
        "    %pip install pipreqs\n",
        "    # for Frechet inception distance\n",
        "    %pip install pytorch-fid\n",
        "\n",
        "    %cd drive/My Drive/optml_gan2\n",
        "    PATH = './' \n",
        "else:\n",
        "    PATH = './' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "M50Mkga4EhNy"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.parallel\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as animation\n",
        "from IPython.display import HTML\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pickle\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQY6pqVIgdUH"
      },
      "source": [
        "# Generate Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YaRkKzQNgdUH"
      },
      "outputs": [],
      "source": [
        "# # converts notebook to .py file for pipreqs\n",
        "# !jupyter nbconvert --output-dir=\"./\" --to script dcgan.ipynb \n",
        "\n",
        "# # creates the requirement.txt file\n",
        "# !pipreqs --force\n",
        "# os.remove('./dcgan.py')  # deletes the .py file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqmZqleaEhN0"
      },
      "source": [
        "# Source code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDrLP1SExqZT"
      },
      "source": [
        "## Data handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NP4dzAueOkhe"
      },
      "outputs": [],
      "source": [
        "# there are problems with downloading CelebA\n",
        "# see https://stackoverflow.com/questions/65528568/how-do-i-load-the-celeba-dataset-on-google-colab-using-torch-vision-without-ru\n",
        "\n",
        "def get_dataset(name, image_size, dataroot):\n",
        "    # torchvision dataset\n",
        "    dataset = None\n",
        "\n",
        "    # number of channels in the training images (3 for color, 1 for grayscale)\n",
        "    nc = None\n",
        "\n",
        "    if name == 'cifar10':\n",
        "        nc = 3\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5) # todo: Why do we use these means and stds ? \n",
        "        )])\n",
        "\n",
        "        dataset = torchvision.datasets.CIFAR10(dataroot, download=True, \n",
        "                                            train=True,  transform=transform)\n",
        "    elif name == 'mnist':\n",
        "        nc = 1\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.CenterCrop(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5), (0.5)) # todo: Why do we use these means and stds and not the mean and std of the dataset?\n",
        "        ])\n",
        "\n",
        "        dataset = torchvision.datasets.MNIST(dataroot, download=True, \n",
        "                                            train=True,  transform=transform)\n",
        "    else:\n",
        "        raise ValueError(\"Unknown dataset name\")\n",
        "    \n",
        "    return dataset, nc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "J0M-_II2wgTC"
      },
      "outputs": [],
      "source": [
        "def plot_img(dataloader, dataset_name):\n",
        "    # Plot some training images\n",
        "    real_batch = next(iter(dataloader))\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Training Images\")\n",
        "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n",
        "    plt.savefig(PATH + 'img/training_images_' + dataset_name, format=\"png\",dpi=400)\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_qwUuFhj_k-"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i_55CgnekALb"
      },
      "outputs": [],
      "source": [
        "def count_parameters(model):\n",
        "    # from https://discuss.pytorch.org/t/how-do-i-check-the-number-of-parameters-of-a-model/4325/9\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def create_repo_paths(PATH):\n",
        "\n",
        "    data_path = PATH + '/data'\n",
        "    generated_data_path = PATH + '/generated_data'\n",
        "    img_path = PATH + '/img'\n",
        "    src_path = PATH + '/src'\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    os.makedirs(generated_data_path, exist_ok=True)\n",
        "    os.makedirs(img_path, exist_ok=True)\n",
        "    os.makedirs(src_path, exist_ok=True)\n",
        "    return\n",
        "# Paths to load and save the models\n",
        "\n",
        "\n",
        "def generate_paths(PATH, extra_word, loss_name, lrD, lrG, beta1, iter_dis, iter_gen, grad_penalty_coef, create_dir):\n",
        "\n",
        "    param_str = loss_name + 'Loss_' + 'lrd' + \\\n",
        "        str(lrD) + '_lrg' + str(lrG) + '_b1' + 'b' + str(beta1)\n",
        "    param_str = param_str + '_itd' + \\\n",
        "        str(iter_dis) + '_itg' + str(iter_gen) + \\\n",
        "        '_gpv' + str(grad_penalty_coef) + '_'\n",
        "\n",
        "    # + str(datetime.date(datetime.now())).replace('-', '_') + \"_\"\n",
        "    experiment_path = PATH + \"generated_data/\" + extra_word + param_str\n",
        "\n",
        "    models_path = experiment_path + '/models/'\n",
        "\n",
        "    stats_path = experiment_path + '/stat.pickle'\n",
        "\n",
        "    if create_dir:\n",
        "        os.makedirs(models_path, exist_ok=True)\n",
        "        os.makedirs(experiment_path, exist_ok=True)\n",
        "\n",
        "    return experiment_path, stats_path, models_path\n",
        "\n",
        "\n",
        "def model_paths(experiment_path, epoch, models_path):\n",
        "\n",
        "    model_name_G = 'model_G_' + str(epoch)\n",
        "    save_path_G = models_path + model_name_G\n",
        "\n",
        "    model_name_D = 'model_D_'+str(epoch)\n",
        "    save_path_D = models_path + model_name_D\n",
        "\n",
        "    return save_path_G, save_path_D\n",
        "\n",
        "\n",
        "def set_seeds(manualSeed=123):\n",
        "    # Set random seed for reproducibility\n",
        "\n",
        "    # manualSeed = random.randint(1, 10000) # use if you want new results\n",
        "    print(\"Random Seed: \", manualSeed)\n",
        "    random.seed(manualSeed)\n",
        "    torch.manual_seed(manualSeed)\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UNmOJSYEhN1"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bjjgdB2YEhN2"
      },
      "outputs": [],
      "source": [
        "# custom weights initialization called on netG and netD\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Sa42nmjtEhN4"
      },
      "outputs": [],
      "source": [
        "def init_net(model, device):\n",
        "    # Create the generator\n",
        "    net = model.to(device)\n",
        "\n",
        "    # Handle multi-gpu if desired\n",
        "    if (device.type == 'cuda') and (ngpu > 1):\n",
        "        net = nn.DataParallel(net, list(range(ngpu)))\n",
        "\n",
        "    # Apply the weights_init function to randomly initialize all weights\n",
        "    #  to mean=0, stdev=0.02.\n",
        "    #net.apply(weights_init)\n",
        "\n",
        "    # Print the model\n",
        "    print(net)\n",
        "    return net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAhsK2LyUnb_"
      },
      "source": [
        "## Losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YUeIrmkvUqV0"
      },
      "outputs": [],
      "source": [
        "# stability constant\n",
        "EPS = 1e-15\n",
        "\n",
        "\n",
        "# KL-divergence\n",
        "def loss_gen_kl(dis_output, eps=EPS):\n",
        "    return - torch.log(dis_output + eps).mean() \n",
        "\n",
        "def loss_dis_kl(dis_output_real, dis_output_fake, eps=EPS):\n",
        "    return - (torch.log(dis_output_real + eps)).mean() - (torch.log(1. - dis_output_fake + eps)).mean()\n",
        "\n",
        "\n",
        "# Wasserstein distance\n",
        "# Requires special output of the network + weight clipping / grad penalty\n",
        "def loss_gen_wasser(dis_output, eps=EPS):\n",
        "    return - dis_output.mean()\n",
        "\n",
        "def loss_dis_wasser(dis_output_real, dis_output_fake, eps=EPS):\n",
        "    return - (dis_output_real.mean() - dis_output_fake.mean())\n",
        "\n",
        "\n",
        "# Hinge loss\n",
        "def loss_gen_hinge(dis_output, eps=EPS):\n",
        "    return - dis_output.mean()\n",
        "\n",
        "def loss_dis_hinge(dis_output_real, dis_output_fake, eps=EPS):\n",
        "    return torch.nn.ReLU()(1.0 - dis_output_real).mean() + torch.nn.ReLU()(1.0 + dis_output_fake).mean()\n",
        "\n",
        "\n",
        "loss_dict = {\n",
        "    \"kl\"    : (loss_dis_kl, loss_gen_kl),\n",
        "    \"wass\"  : (loss_dis_wasser, loss_gen_wasser),\n",
        "    \"hinge\" : (loss_dis_hinge, loss_gen_hinge)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwd6NVO7zoze"
      },
      "source": [
        "## FID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJpwdqHN1lN-"
      },
      "source": [
        "https://www.kaggle.com/code/ibtesama/gan-in-pytorch-with-fid/notebook  \n",
        "https://github.com/mseitzer/pytorch-fid   \n",
        "currently uses the kaggle stuff since calculating FID on the repo is run from commandline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "YxtQ5pk4z_0M"
      },
      "outputs": [],
      "source": [
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False):\n",
        "        \n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        \n",
        "        inception = models.inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fIkLqEY-zpQm"
      },
      "outputs": [],
      "source": [
        "def calculate_activation_statistics(images,inception_model,batch_size=128, dims=2048,\n",
        "                    cuda=False):\n",
        "    inception_model.eval()\n",
        "    act=np.empty((len(images), dims))\n",
        "    \n",
        "    if cuda:\n",
        "        batch=images.cuda()\n",
        "    else:\n",
        "        batch=images\n",
        "\n",
        "    pred = inception_model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
        "    \n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "heCyV4qHzu3D"
      },
      "outputs": [],
      "source": [
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    \n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    \n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1) +\n",
        "            np.trace(sigma2) - 2 * tr_covmean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "uBzRjX6Bzu06"
      },
      "outputs": [],
      "source": [
        "def calculate_frechet(images_real,images_fake,model):\n",
        "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
        "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
        "    \n",
        "     \"\"\"get fretchet distance\"\"\"\n",
        "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
        "     return fid_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfjSFPjEyGg_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "V0i7nSDwyFy9"
      },
      "outputs": [],
      "source": [
        "def init_optimizers(optimizer_name, netD, netG, lrD, lrG, beta1, nz, device, \n",
        "                    momentumD, momentumG):\n",
        "    # Create batch of latent vectors that we will use to visualize\n",
        "    #  the progression of the generator\n",
        "    fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
        "\n",
        "    # todo: outdated now\n",
        "    # Establish convention for real and fake labels during training\n",
        "    real_label = 1.\n",
        "    fake_label = 0.\n",
        "     \n",
        "    if optimizer_name =='adam': \n",
        "        # Setup Adam optimizers for both G and D\n",
        "        optimizerD = optim.Adam(netD.parameters(), lr=lrD, betas=(beta1, 0.999))\n",
        "        optimizerG = optim.Adam(netG.parameters(), lr=lrG, betas=(beta1, 0.999))\n",
        "    if optimizer_name =='sgd':\n",
        "        optimizerD = optim.SGD(netD.parameters(), lr=lrD, momentum=momentumD, dampening=0, weight_decay=0)\n",
        "        optimizerG = optim.SGD(netG.parameters(), lr=lrG, momentum=momentumG, dampening=0, weight_decay=0)\n",
        "    if optimizer_name =='rmsprop':\n",
        "        optimizerD = optim.RMSprop(netD.parameters(), lr=lrD, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "        optimizerG = optim.RMSprop(netG.parameters(), lr=lrG, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)\n",
        "       \n",
        "    return fixed_noise, real_label, fake_label, optimizerD, optimizerG\n",
        "\n",
        "def init_losses(loss_type):\n",
        "    if loss_type not in loss_dict.keys():\n",
        "        raise Exception(\"Unknown loss type\")\n",
        "\n",
        "    return loss_dict[loss_type]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2DyNJjV7IQif"
      },
      "outputs": [],
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "def gradient_penalty(device, discriminator, data_gen, data_real, lambda_reg=0.1):\n",
        "    alpha = torch.rand(data_real.shape[0], 1).to(device)\n",
        "    dims_to_add = len(data_real.size()) - 2\n",
        "    for i in range(dims_to_add):\n",
        "        alpha = alpha.unsqueeze(-1)\n",
        "\n",
        "    interpolates = (alpha * data_real + ((1. - alpha) * data_gen)).to(device)\n",
        "\n",
        "    interpolates = Variable(interpolates, requires_grad=True)\n",
        "\n",
        "    disc_interpolates = discriminator(interpolates)\n",
        "    grad_outputs = torch.ones(disc_interpolates.size()).to(device)\n",
        "\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=disc_interpolates, inputs=interpolates, grad_outputs=grad_outputs,\n",
        "        create_graph=True, retain_graph=True, only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    grad_penalty_coef = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * lambda_reg\n",
        "\n",
        "    return grad_penalty_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ktvJ-tAULJuc"
      },
      "outputs": [],
      "source": [
        "def discriminator_step(optimizerD, f_loss_dis, netD, netG, data, device, real_label, fake_label, gp_coef):\n",
        "    netD.zero_grad()\n",
        "\n",
        "    real_cpu = data[0].to(device)\n",
        "    b_size = real_cpu.size(0)\n",
        "    label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n",
        "    output_real = netD(real_cpu).view(-1)\n",
        "\n",
        "    noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "    fake = netG(noise)\n",
        "    label.fill_(fake_label)\n",
        "    output_fake = netD(fake.detach()).view(-1)\n",
        "\n",
        "    errD = f_loss_dis(output_real, output_fake)\n",
        "\n",
        "    if gp_coef != 0.0:\n",
        "        errD += gp_coef * gradient_penalty(device, netD, fake, real_cpu)\n",
        "\n",
        "    errD.backward()\n",
        "    optimizerD.step()\n",
        "\n",
        "    D_x = output_real.mean().item()\n",
        "    D_G_z1 = output_fake.mean().item()\n",
        "\n",
        "    return D_x, D_G_z1, errD, label, fake, real_cpu\n",
        "\n",
        "\n",
        "def generator_step(optimizerG, f_loss_gen, netD, netG, label, fake, real_label):\n",
        "    netG.zero_grad()\n",
        "    output = netD(fake).view(-1)\n",
        "    \n",
        "    errG = f_loss_gen(output)\n",
        "    errG.backward()\n",
        "    \n",
        "    D_G_z2 = output.mean().item()\n",
        "\n",
        "    optimizerG.step()\n",
        "    return D_G_z2, errG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ezKphnm8gdUP"
      },
      "outputs": [],
      "source": [
        "class Training:\n",
        "    def __init__(self, loss_name, netD, netG, device, real_label, fake_label, dataloader, num_epochs,\n",
        "                 fixed_noise, lrD, lrG, beta1, experiment_prefix, save_models, \n",
        "                 PATH, save_stats, create_dir, iter_per_epoch_dis, iter_per_epoch_gen, grad_penalty_coef,\n",
        "                 optimizerD, optimizerG,\n",
        "                 save_epochs=10):\n",
        "        self.optimizerD, self.optimizerG = optimizerD, optimizerG\n",
        "\n",
        "        self.loss_name = loss_name\n",
        "        self.netD, self.netG = netD, netG\n",
        "        self.device = device\n",
        "        self.real_label, self.fake_label = real_label, fake_label\n",
        "        self.dataloader = dataloader\n",
        "        self.num_epochs = num_epochs\n",
        "        self.fixed_noise = fixed_noise\n",
        "        self.iter_per_epoch_dis, self.iter_per_epoch_gen = iter_per_epoch_dis, iter_per_epoch_gen\n",
        "        self.grad_penalty_coef = grad_penalty_coef\n",
        "\n",
        "        self.save_models = save_models\n",
        "        self.PATH = PATH\n",
        "        self.experiment_prefix = experiment_prefix\n",
        "        self.loss_name = loss_name\n",
        "        self.lrD = lrD\n",
        "        self.lrG = lrG\n",
        "        self.beta1 = beta1\n",
        "        self.create_dir = create_dir\n",
        "        self.save_stats = save_stats\n",
        "        self.save_epochs = save_epochs\n",
        "        self.experiment_path, self.stats_path, self.models_path = generate_paths(self.PATH, self.experiment_prefix, \n",
        "                                                          self.loss_name, self.lrD, self.lrG, \n",
        "                                                          self.beta1, self.iter_per_epoch_dis, self.iter_per_epoch_gen, \n",
        "                                                          self.grad_penalty_coef, self.create_dir)\n",
        "        \n",
        "        \n",
        "        \n",
        "\n",
        "        # self.inception_model = inception_model\n",
        "    def _output_training_stats(self, epoch, i, size, errD, errG, D_x, D_G_z1, D_G_z2, t0):\n",
        "        if i == size:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f t: %2.3f'\n",
        "                % (epoch, self.num_epochs, i, len(self.dataloader),\n",
        "                    errD, errG, D_x, D_G_z1, D_G_z2, time.time()-t0))\n",
        "        elif i % 50 == 0:\n",
        "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
        "                % (epoch, self.num_epochs, i, len(self.dataloader),\n",
        "                    errD, errG, D_x, D_G_z1, D_G_z2))\n",
        "    \n",
        "    def _save_gen_output(self, iters, epoch, i):\n",
        "        if (iters % 500 == 0) or ((epoch == self.num_epochs-1) and (i == len(self.dataloader)-1)):\n",
        "            with torch.no_grad():\n",
        "                fake = self.netG(self.fixed_noise).detach().cpu()\n",
        "            self.img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
        "            # maybe remove this if we have memory problems\n",
        "            self.img_list_nogrid.append(fake)\n",
        "\n",
        "    def train(self):\n",
        "        f_loss_dis, f_loss_gen = init_losses(self.loss_name)\n",
        "\n",
        "        G_losses, D_losses = [], []\n",
        "        self.img_list = []\n",
        "        self.img_list_nogrid = []\n",
        "\n",
        "        iters = 0\n",
        "\n",
        "        print(\"Starting Training Loop...\")\n",
        "\n",
        "        D_x, D_G_z1, errD, label, fake = None, None, None, None, None\n",
        "        D_G_z2, errG = None, None\n",
        "\n",
        "        for epoch in range(self.num_epochs):\n",
        "            t0 = time.time()\n",
        "\n",
        "            \n",
        "\n",
        "            for i, data in enumerate(self.dataloader, 0):\n",
        "                for _ in range(self.iter_per_epoch_dis):\n",
        "                    D_x, D_G_z1, errD, label, fake, real_cpu = discriminator_step(self.optimizerD,\n",
        "                        f_loss_dis, self.netD, self.netG, data, self.device, \n",
        "                        self.real_label, self.fake_label, self.grad_penalty_coef\n",
        "                    )\n",
        "\n",
        "                for _ in range(self.iter_per_epoch_gen):\n",
        "                    D_G_z2, errG = generator_step(self.optimizerG,\n",
        "                        f_loss_gen, self.netD, self.netG, label, fake, self.real_label\n",
        "                    )\n",
        "                \n",
        "                # Save Losses for plotting later\n",
        "                G_losses.append(errG.item())\n",
        "                D_losses.append(errD.item())\n",
        "\n",
        "                size = len(self.dataloader) - 1\n",
        "                self._output_training_stats(epoch, i, size, errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, t0)\n",
        "                \n",
        "                # Check how the generator is doing by saving G's output on fixed_noise\n",
        "                self._save_gen_output(iters, epoch, i)\n",
        "                \n",
        "                iters += 1\n",
        "\n",
        "            # save the model every self.save_epochs epochs\n",
        "            if self.save_models and (epoch%self.save_epochs == 0):\n",
        "                self.save_path_G, self.save_path_D = model_paths(self.experiment_path, epoch, self.models_path)\n",
        "                save_models(self.netG, self.netD, self.save_path_G, self.save_path_D)\n",
        "\n",
        "        stats = {\n",
        "            'img_list' : self.img_list,\n",
        "            'img_list_nogrid' : self.img_list_nogrid,\n",
        "            'G_losses' : G_losses,\n",
        "            'D_losses' : D_losses  \n",
        "        }\n",
        "        # save stats at the end of training\n",
        "        if self.save_stats:\n",
        "            pickle_save(stats, self.stats_path)\n",
        "\n",
        "        return stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76HIJHth6szw"
      },
      "source": [
        "## Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ZDqpaE6i6uLu"
      },
      "outputs": [],
      "source": [
        "def plot_loss(G_losses, D_losses, save = False):\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.title(\"Generator and Discriminator Loss During Training\")\n",
        "    plt.plot(G_losses, label=\"G\")\n",
        "    plt.plot(D_losses, label=\"D\")\n",
        "    plt.xlabel(\"iterations\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    if save == True:\n",
        "      plt.savefig(PATH + 'img/loss', format=\"png\",dpi=400)\n",
        "\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "OMjT45Nw7BBf"
      },
      "outputs": [],
      "source": [
        "def plot_realvsfake(dataloader, device, img_list, save = False):\n",
        "    # Grab a batch of real images from the dataloader\n",
        "    real_batch = next(iter(dataloader))\n",
        "\n",
        "    # Plot the real images\n",
        "    plt.figure(figsize=(15,15))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Real Images\")\n",
        "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
        "\n",
        "    # Plot the fake images from the last epoch\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Fake Images\")\n",
        "    plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
        "    if save == True:\n",
        "        plt.savefig(PATH + 'img/real_vs_fake', format=\"png\",dpi=400)\n",
        "    plt.show()\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFOlQTaL7ibw"
      },
      "source": [
        "## Serialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x9OTjxEH7rg6"
      },
      "outputs": [],
      "source": [
        "def save_models(netG ,netD, save_path_G, save_path_D):\n",
        "    torch.save(netG.state_dict(), save_path_G)\n",
        "    torch.save(netD.state_dict(), save_path_D)\n",
        "    print('GAN saved')\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "TSCzFlDT7kE3"
      },
      "outputs": [],
      "source": [
        "def load_models(ngpu, Discriminator, Generator, save_path_G, save_path_D):\n",
        "  \n",
        "    netD = init_net(Discriminator(ngpu, nc, loss_name), device)\n",
        "    netD.load_state_dict(torch.load(save_path_D))\n",
        "    netD.eval()\n",
        "\n",
        "    netG = init_net(Generator(ngpu, nc, nz), device)\n",
        "    netG.load_state_dict(torch.load(save_path_G))\n",
        "    netG.eval()\n",
        "    \n",
        "    print('GAN loaded')\n",
        "    return netD, netG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ukW6LisRmoxO"
      },
      "outputs": [],
      "source": [
        "def save_dict(dict, dict_path):\n",
        "    with open(dict_path, 'w') as file:\n",
        "        file.write(json.dumps(dict))  \n",
        "    return\n",
        "\n",
        "def read_dict(dict_path):\n",
        "    with open(dict_path) as f:\n",
        "        data = f.read()\n",
        "    data = json.loads(data)\n",
        "    return data\n",
        "\n",
        "def pickle_save(something, path):\n",
        "    with open(path, 'wb') as handle:\n",
        "        pickle.dump(something, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "def pickle_load (path):\n",
        "    with open(path, 'rb') as handle:\n",
        "        something = pickle.load(handle)\n",
        "    return something"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cjD_s6dFNzL"
      },
      "source": [
        "## Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxcNcsuwIgYu"
      },
      "source": [
        "https://keras.io/examples/generative/conditional_gan/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "BJcokCcO5E0d"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu, nc, nz):\n",
        "        super(Generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(nz, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, nc * 28 * 28)\n",
        "        return    \n",
        "\n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = x.reshape([x.shape[0], -1])\n",
        "\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = torch.tanh(self.fc4(x))\n",
        "        x = x.reshape((-1, nc, 28, 28))\n",
        "        return x\n",
        "    \n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, ngpu, nc, loss):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(28 * 28, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "\n",
        "        if loss == \"kl\":\n",
        "            # for KL - discriminator is a classifier\n",
        "            self.act = torch.sigmoid\n",
        "        else:\n",
        "            # for Wasserstein and hinge - discriminator is a critic\n",
        "            self.act = lambda x: x\n",
        "\n",
        "        return    \n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.reshape([x.shape[0], -1])\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = self.act(self.fc4(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpIukUkR5ny_"
      },
      "source": [
        "# Hyper-parameter optimization (gridsearch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "N7dlYcZspm9i"
      },
      "outputs": [],
      "source": [
        "def set_loss_params(loss_name):\n",
        "    \n",
        "    iter_dis, iter_gen, grad_penalty_coef = 1, 1, 0.0\n",
        "\n",
        "    if loss_name == \"wass\":\n",
        "        iter_dis, grad_penalty_coef = 5, 10.0\n",
        "\n",
        "    return iter_dis, iter_gen, grad_penalty_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "9Nwk5FBy5nbU"
      },
      "outputs": [],
      "source": [
        "def run_experiment(ngpu, device, dataset, workers,\n",
        "                   batch_size, shuffle, num_epochs, plot, lrD, lrG, beta1, nz, \n",
        "                   loss_name, experiment_prefix, save_stats, create_dir, \n",
        "                   iter_per_epoch_dis, iter_per_epoch_gen, grad_penalty_coef, \n",
        "                   save_epochs, save_models, momentumD, momentumG, optimizer_name): \n",
        "\n",
        "\n",
        "    netG = init_net(Generator(ngpu, nc, nz), device)\n",
        "    netD = init_net(Discriminator(ngpu, nc, loss_name), device)\n",
        "    \n",
        "    # Create the dataloader\n",
        "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                            shuffle=True, num_workers=workers)\n",
        "     \n",
        "    fixed_noise, real_label, fake_label, optimizerD, optimizerG = init_optimizers(optimizer_name, netD, netG, lrD, lrG, beta1, nz, device, momentumD, momentumG)\n",
        "\n",
        "    experiment_prefix = experiment_prefix + optimizer_name +'_mG'+str(momentumD) +'_mD'+str(momentumG) + '_'\n",
        "\n",
        "    gan_training = Training(loss_name, netD, netG, device, real_label, fake_label, \n",
        "                            dataloader, num_epochs, fixed_noise, \n",
        "                            lrD, lrG, beta1, experiment_prefix, save_models, PATH, save_stats, create_dir,\n",
        "                            iter_per_epoch_dis, iter_per_epoch_gen, grad_penalty_coef, optimizerD, optimizerG, save_epochs=save_epochs)\n",
        "\n",
        "    stats = gan_training.train()\n",
        "    \n",
        "    if plot:\n",
        "        plot_loss(G_losses, D_losses, save = False)\n",
        "        plot_realvsfake(dataloader, device, img_list, save = False)\n",
        "    \n",
        "    return stats, dataloader, netG, netD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "QbgPKxI2Jzfq"
      },
      "outputs": [],
      "source": [
        "def grid_search(ngpu, device, dataset, workers, \n",
        "                experiment_prefix, batch_size_list, shuffle_list, \n",
        "                num_epochs_list, loss_name_list, optimizer_name_list, \n",
        "                beta1_list, lr_list, momentums_list, plot, save_stats, create_dir\n",
        "                , save_epochs, save_models, manualSeed):\n",
        "\n",
        "    # all_stats = [] \n",
        "\n",
        "    # TO DO change for loops to zip \n",
        "    for batch_size in batch_size_list:\n",
        "        for shuffle in shuffle_list:\n",
        "            for num_epochs in num_epochs_list:\n",
        "                for loss_name in loss_name_list:\n",
        "\n",
        "                    iter_per_epoch_dis, iter_per_epoch_gen, grad_penalty_coef = set_loss_params(loss_name)\n",
        "                    for optimizer_name in optimizer_name_list:\n",
        "                        for beta1 in beta1_list:\n",
        "                            for lr in lr_list:\n",
        "                                for (momentumD, momentumG) in momentums_list : \n",
        "                                    lrD = lr\n",
        "                                    lrG = lr\n",
        "                                    # set seed before every experiment\n",
        "                                    set_seeds(manualSeed = manualSeed)\n",
        "\n",
        "                                    \n",
        "\n",
        "                                    print('====================PARAMETERS===================')\n",
        "                                    print('batch_size =', batch_size)\n",
        "                                    print('shuffle =', shuffle)\n",
        "                                    print('num_epoch =', num_epochs)\n",
        "                                    print('loss_name =', loss_name)\n",
        "                                    print('optimizer_name =', optimizer_name)\n",
        "                                    print('beta1 =', beta1)\n",
        "                                    print('lr =', lr)\n",
        "                                    print('iter_per_epoch_dis =', iter_per_epoch_dis)\n",
        "                                    print('iter_per_epoch_gen =', iter_per_epoch_gen)\n",
        "                                    print('grad_penalty_coef =', grad_penalty_coef)\n",
        "\n",
        "                                    stats, dataloader, netG, netD = run_experiment(ngpu, device, dataset, workers,\n",
        "                                                                                  batch_size, shuffle, num_epochs, plot, lrD, lrG, beta1, nz, \n",
        "                                                                                  loss_name, experiment_prefix, save_stats, create_dir, \n",
        "                                                                                  iter_per_epoch_dis, iter_per_epoch_gen, grad_penalty_coef, \n",
        "                                                                                  save_epochs, save_models, momentumD, momentumG, optimizer_name)\n",
        "\n",
        "    # all_stats.append(stats)\n",
        "    \n",
        "    return #all_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "NTdcMZ4AtiVw"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = PATH + \"data/\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = 'mnist' # 'cifar10' or 'mnist'\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
        "image_size = 28 # 28 for mnist, 64 for others\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 128\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "create_repo_paths(PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DRqoAVcKtC05"
      },
      "outputs": [],
      "source": [
        "dataset, nc = get_dataset(dataset_name, image_size, dataroot)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEmdQBI_LFcW",
        "outputId": "e780fcb6-e77b-44bf-b914-5f02fba4471e"
      },
      "outputs": [],
      "source": [
        "grid_search(ngpu, device, dataset, workers,\n",
        "                        experiment_prefix = '', # and an extra word at the begining to the save path of the models and stats\n",
        "                        batch_size_list = [128],\n",
        "                        shuffle_list = [True],\n",
        "                        num_epochs_list = [300],\n",
        "                        loss_name_list = ['wass'], # wass, hinge\n",
        "                        optimizer_name_list = ['adam'], # 'adam' 'sgd' 'rmsprop'\n",
        "                        beta1_list = [0.9], # 0.9 == default # Beta1 hyperparam for Adam optimizers\n",
        "                        lr_list = [1e-1,1e-2,1e-3,1e-4, 1e-5, 1e-6, 1e-7], # [1,1e-3,2e-4,1e-5,1e-6]\n",
        "                        momentums_list = [(0,0)], # [(momentumD, momentumG)]\n",
        "                        plot = False,\n",
        "                        save_stats = True, # save the stats to disk\n",
        "                        create_dir = True, # create the directories to save files\n",
        "                        save_epochs = 10, # save the model every save_epochs epochs\n",
        "                        save_models = True, # save the models to disk\n",
        "                        manualSeed = 123 # keep at 123\n",
        "                        ) # grad_penalty_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search(ngpu, device, dataset, workers,\n",
        "                        experiment_prefix = '', # and an extra word at the begining to the save path of the models and stats\n",
        "                        batch_size_list = [128],\n",
        "                        shuffle_list = [True],\n",
        "                        num_epochs_list = [300],\n",
        "                        loss_name_list = ['wass'], # wass, hinge\n",
        "                        optimizer_name_list = ['sgd'], # 'adam' 'sgd' 'rmsprop'\n",
        "                        beta1_list = [0.9], # 0.9 == default # Beta1 hyperparam for Adam optimizers\n",
        "                        lr_list = [1e-1,1e-2,1e-3,1e-4, 1e-5, 1e-6, 1e-7], # [1,1e-3,2e-4,1e-5,1e-6]\n",
        "                        momentums_list = [(0,0)], # [(momentumD, momentumG)]\n",
        "                        plot = False,\n",
        "                        save_stats = True, # save the stats to disk\n",
        "                        create_dir = True, # create the directories to save files\n",
        "                        save_epochs = 10, # save the model every save_epochs epochs\n",
        "                        save_models = True, # save the models to disk\n",
        "                        manualSeed = 123 # keep at 123\n",
        "                        ) # grad_penalty_coef"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "grid_search(ngpu, device, dataset, workers,\n",
        "                        experiment_prefix = '', # and an extra word at the begining to the save path of the models and stats\n",
        "                        batch_size_list = [128],\n",
        "                        shuffle_list = [True],\n",
        "                        num_epochs_list = [300],\n",
        "                        loss_name_list = ['wass','hinge'], # wass, hinge\n",
        "                        optimizer_name_list = ['rmsprop'], # 'adam' 'sgd' 'rmsprop'\n",
        "                        beta1_list = [0.9], # 0.9 == default # Beta1 hyperparam for Adam optimizers\n",
        "                        lr_list = [1e-1,1e-2,1e-3,1e-4, 1e-5, 1e-6, 1e-7], # [1,1e-3,2e-4,1e-5,1e-6]\n",
        "                        momentums_list = [(0,0)], # [(momentumD, momentumG)]\n",
        "                        plot = False,\n",
        "                        save_stats = True, # save the stats to disk\n",
        "                        create_dir = True, # create the directories to save files\n",
        "                        save_epochs = 10, # save the model every save_epochs epochs\n",
        "                        save_models = True, # save the models to disk\n",
        "                        manualSeed = 123 # keep at 123\n",
        "                        ) # grad_penalty_coef"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beMBvkIjFl7z"
      },
      "source": [
        "# Training example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###########\n",
        " TO DO : run one training iteration with the examples and "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSQF8-Mitrs2"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VeYrHwkRx-59"
      },
      "outputs": [],
      "source": [
        "# Root directory for dataset\n",
        "dataroot = PATH + \"data/\"\n",
        "\n",
        "# Dataset name\n",
        "dataset_name = 'mnist' # 'cifar10' or 'mnist'\n",
        "\n",
        "# Number of workers for dataloader\n",
        "workers = 2\n",
        "\n",
        "# Batch size during training\n",
        "batch_size = 128\n",
        "\n",
        "# Spatial size of training images. All images will be resized to this size using a transformer.\n",
        "image_size = 28 # 28 for mnist, 64 for others\n",
        "\n",
        "# Size of z latent vector (i.e. size of generator input)\n",
        "nz = 128\n",
        "\n",
        "# Number of GPUs available. Use 0 for CPU mode.\n",
        "ngpu = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROhmdtOztq8W"
      },
      "outputs": [],
      "source": [
        "# Number of training epochs\n",
        "num_epochs = 3\n",
        "\n",
        "# Learning rate for optimizers\n",
        "lrD = 2e-4\n",
        "lrG = 2e-4\n",
        "\n",
        "# Beta1 hyperparam for Adam optimizers\n",
        "beta1 = 0.9 # 0.9 == default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uvDLoiTmyC8b"
      },
      "outputs": [],
      "source": [
        "dataset, nc = get_dataset(dataset_name, image_size, dataroot)\n",
        "\n",
        "# Create the dataloader\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
        "                                         shuffle=True, num_workers=workers)\n",
        "\n",
        "# Decide which device we want to run on\n",
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
        "\n",
        "#print(dataset)\n",
        "#plot_img(dataloader, dataset_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1rZFBRsvj-e"
      },
      "outputs": [],
      "source": [
        "loss_name = \"wass\" # wass, hinge\n",
        "iter_dis, iter_gen, grad_penalty_coef = 1, 1, 0.0\n",
        "\n",
        "if loss_name == \"wass\":\n",
        "    iter_dis, grad_penalty_coef = 5, 10.0\n",
        "\n",
        "netG = init_net(Generator(ngpu, nc, nz), device)\n",
        "print('Generator parameters', count_parameters(netG))\n",
        "\n",
        "netD = init_net(Discriminator(ngpu, nc, loss_name), device)\n",
        "print('Discriminator parameters', count_parameters(netD))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0wNXW77zD4b"
      },
      "source": [
        "## Run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ-zyO5918Df"
      },
      "outputs": [],
      "source": [
        "fixed_noise, real_label, fake_label, optimizerD, optimizerG = init_optimizers(netD, netG, lrD, lrG, beta1, nz, device)\n",
        "experiment_prefix = '' # and extra word to add the automatically generate one if you really need it, ideally keep empty\n",
        "gan_training = Training(loss_name, netD, netG, device, real_label, fake_label, \n",
        "                        dataloader, num_epochs, fixed_noise, \n",
        "                        grad_penalty_coef, lrD, lrG, beta1, experiment_prefix, save_models, PATH, save_stats = True, create_dir=True,\n",
        "                        iter_per_epoch_dis=1, iter_per_epoch_gen=1, grad_penalty_coef=0.0)\n",
        "\n",
        "stats = gan_training.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhfcOvw-GTCX"
      },
      "outputs": [],
      "source": [
        "img_list = stats['img_list']\n",
        "G_losses = stats['G_losses']\n",
        "D_losses = stats['D_losses']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python -m pytorch_fid path/to/dataset1 path/to/dataset2 \n",
        "\n",
        "\n",
        "def bla(folderlist):\n",
        "    for item in folderlist:\n",
        "        !python -m pytorch_fid item[1] item[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-URPzv_TEhN7"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQ9voI50JsfC"
      },
      "outputs": [],
      "source": [
        "plot_loss(G_losses, D_losses, save = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrhkQ1fwJrJa"
      },
      "outputs": [],
      "source": [
        "plot_realvsfake(dataloader, device, img_list, save = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA4p6YNOEhN7"
      },
      "source": [
        "## G’s progression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rI3SO9R1EhN7"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "fig = plt.figure(figsize=(8,8))\n",
        "plt.axis(\"off\")\n",
        "ims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\n",
        "ani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n",
        "\n",
        "HTML(ani.to_jshtml())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaTJWVJPF1CQ"
      },
      "source": [
        "# Serialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSHhTbJGBDMS"
      },
      "outputs": [],
      "source": [
        "epoch = 999999999\n",
        "experiment_prefix = 'tteeesst'\n",
        "experiment_path, stats_path, models_path = generate_paths(PATH, experiment_prefix, loss_name, lrD, lrG, beta1, iter_dis, iter_gen, grad_penalty_coef, create_dir=True)\n",
        "save_path_G, save_path_D = model_paths(experiment_path, epoch, models_path)\n",
        "\n",
        "print(experiment_path)\n",
        "print(stats_path)\n",
        "print(save_path_G)\n",
        "print(save_path_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OObYCZyxmO2a"
      },
      "outputs": [],
      "source": [
        "save_models(netG ,netD, save_path_G, save_path_D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nmoTYEInkMF"
      },
      "outputs": [],
      "source": [
        "pickle_save(stats, stats_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6_MWpaCaKvxx"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "netD, netG = load_models(ngpu, Discriminator, Generator, save_path_G, save_path_D)\n",
        "\n",
        "stats = pickle_load(stats_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M21gaXFb05x8"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cN_wInEf1trd"
      },
      "outputs": [],
      "source": [
        "def sample_gen(netG, nz, data):\n",
        "    with torch.no_grad():\n",
        "      real_cpu = data[0].to(device)\n",
        "      b_size = real_cpu.size(0)\n",
        "\n",
        "      noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
        "      fake = netG(noise)\n",
        "    return real_cpu, fake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtDAYmuw04wd"
      },
      "outputs": [],
      "source": [
        "# Create the dataloader\n",
        "\n",
        "batch_size_eval = 100\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size_eval,\n",
        "                                         shuffle=True, num_workers=workers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ndW2gBr6dAm"
      },
      "outputs": [],
      "source": [
        "# Load inception model\n",
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "inception_model = InceptionV3([block_idx])\n",
        "inception_model = inception_model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG0cEgL71_6O"
      },
      "outputs": [],
      "source": [
        "# take first batch from the dataloader to get 500 samples :\n",
        "with torch.no_grad():\n",
        "    sample_batch = next(iter(dataloader))  \n",
        "\n",
        "    real_cpu, fake = sample_gen(netG, nz, sample_batch)\n",
        "\n",
        "    t_frechet = time.time()\n",
        "    frechet_dist = calculate_frechet(real_cpu, fake, inception_model) \n",
        "    print('frechet dist:', frechet_dist,'| time to calculate :',time.time()-t_frechet,'s')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "-URPzv_TEhN7"
      ],
      "include_colab_link": true,
      "name": "dcgan.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 ('optml')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "53461a9fb59a29bcaa7e1cfe6b9fe9c30f7f07aae145e77433631c890399cf7b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
